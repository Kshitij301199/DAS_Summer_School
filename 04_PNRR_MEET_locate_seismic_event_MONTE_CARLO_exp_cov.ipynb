{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook for locating a seismic event using McMC sampling\n",
    "\n",
    "### TASK:\n",
    "\n",
    "Locate a seismic event within an homogeneous half-space, using P- and S- wave arrival times. Hypocentral parameters will be found using a \"Markov chain Monte Carlo\" methodology, and including a full Error Covariance matrix (necessary for DAS data)\n",
    "\n",
    "### WORKFLOW:\n",
    "\n",
    "(1) Define data and model space (i.e. prior information about hypocenter parameters)\n",
    "\n",
    "(2) Define the Error Covariance matrix\n",
    "\n",
    "(3) Start the McMC sampling form a random point in the model space\n",
    "\n",
    "(4) Compute Likelihood of the starting model\n",
    "\n",
    "#### Iterate fo N steps in the Markov chain:\n",
    "\n",
    "(3) Modify the _current model_ (the starting model in the first iteration) proposing a _candidate model_ from the prior information\n",
    "\n",
    "(4) Compute the Likelihood of the candidate model\n",
    "\n",
    "(5) Accept or Reject candidate model using Metropolis rule\n",
    "\n",
    "(5.1) If Accepted, the candidate model becomes the current model\n",
    "(5.2) Id Rejected, the current model is kept and the candidate is removed from the chain\n",
    "\n",
    "(6) Every 100 steps in the chain: store the current model\n",
    "\n",
    "(7) Back to (3)\n",
    "\n",
    "#### Post Processing:\n",
    "\n",
    "(8) Extract relevant estimators of the hypocentral parameters from the stored models (eg mean and standard deviation) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib.image as image \n",
    "\n",
    "\n",
    "#For the McMC sampling\n",
    "from mcmc_locate_exp_cov import npa_candidate_model\n",
    "from mcmc_locate_exp_cov import npa_log_likelihood\n",
    "from mcmc_locate_exp_cov import npa_LOCATE_syn\n",
    "from mcmc_locate_exp_cov import npa_metropolis\n",
    "from mcmc_locate_exp_cov import npa_get_starting_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Selected event: SHOT_01\n",
      " Selected FO cable: input/tito_cable_geometry.csv\n",
      " Selected data: data/picks/event.csv\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "###########################################################################################\n",
    "#\n",
    "# EDIT HERE ---- Baribles used in the Jnotebook for the analysis\n",
    "#\n",
    "#\n",
    "#Seismic source\n",
    "EVENT_NAME=\"SHOT_01\"                        # LABEL for creating files\n",
    "event_X=15.5                                # Approx X coord of the source\n",
    "event_Y=6.5                                 # Approx Y coord of the source\n",
    "datafile=\"data/picks/event.csv\"             # P-wave arrival time at DAS channels \n",
    "cablegeom=\"input/tito_cable_geometry.dat\"   # Coordinates of the sensing FO channels\n",
    "#\n",
    "############################################################################################\n",
    "#\n",
    "# Derived variables\n",
    "#\n",
    "# 2D area where the seismic source will be located \n",
    "xmin_mcmc=event_X-1\n",
    "xmax_mcmc=event_X+1\n",
    "ymin_mcmc=event_Y-1\n",
    "ymax_mcmc=event_Y+1\n",
    "#\n",
    "# Zoom close to the true location of the seismic source\n",
    "xmin_zoom=event_X-1\n",
    "xmax_zoom=event_X+1\n",
    "ymin_zoom=event_Y-1\n",
    "ymax_zoom=event_Y+1\n",
    "#\n",
    "#\n",
    "print(' Selected event: ' + EVENT_NAME)\n",
    "print(' Selected FO cable: ' + cablegeom)\n",
    "print(' Selected data: ' + datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map of selected FO cable and approx shot location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data related to selected Shot (i.e. arrival times of P- wave at DAS channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "statfile = open(datafile, 'r')\n",
    "\n",
    "stlo, stla, Ppick = np.loadtxt(datafile, delimitator=',', dtype=(float, int), usecols=(3,4,2), unpack=True)\n",
    "\n",
    "nstat_sele=len(stlo)\n",
    "min_P_pick = float(min(Ppick))\n",
    "print('Found', nstat_sele, 'stations with associated picking -- earliest P-arrival:', min_P_pick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate the seismic source using a Markov chain Monte Carlo approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "#\n",
    "# EDIT HERE\n",
    "#\n",
    "# Define the model space\n",
    "# (Uniform prior probability distirbution)\n",
    "#\n",
    "xmin=xmin_mcmc  # in m\n",
    "xmax=xmax_mcmc  # in m\n",
    "ymin=ymin_mcmc  # in m\n",
    "ymax=ymax_mcmc  # in m\n",
    "zmin=-10        # in m\n",
    "zmax=0          # in m\n",
    "tmin=-0.1       # in s\n",
    "tmax=0.1        # in s\n",
    "vpmin=500.0     # in m/s\n",
    "vpmax=3000.0    # in m/s\n",
    "\n",
    "#\n",
    "#\n",
    "#   MARKOV CHAIN MONTE CARLO PARAMETERS\n",
    "# \n",
    "N_CHAINS=10                        # Number of chains\n",
    "N_MOD_MCMC=20000                   # Number of samples collected alon the McMC\n",
    "BURN_IN=10000                       # Samples discarded at the beginning of the McMC\n",
    "PRIOR = 0                          # PRIOR SAMPLING: 1- sampling prior information; 0- Monte Carlo sampling \n",
    "#\n",
    "#\n",
    "#   EXPONENTIAL COVARIANCE MATRIX DEFINITIONS\n",
    "#\n",
    "std0=0.01\n",
    "r_corr=0.0\n",
    "#\n",
    "#\n",
    "# RANDOM SEED -- to produce always the same sequence of pseudo-random numbers\n",
    "#\n",
    "rseed=210728\n",
    "rng = np.random.default_rng(rseed)\n",
    "#\n",
    "#######################################################\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "\n",
    "#######################################################\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "#\n",
    "# Pre-compute inverse of EXP cov matrix\n",
    "#\n",
    "\n",
    "cor = np.zeros((nstat_sele,nstat_sele), dtype=float)\n",
    "icov = np.zeros((nstat_sele,nstat_sele), dtype=float)\n",
    "\n",
    "ir=0\n",
    "while ir < nstat_sele:\n",
    "    \n",
    "    if ir == 0:\n",
    "        diag0=1.0+(r_corr**2)\n",
    "    if ir == 1:\n",
    "        diag0=-1.0*r_corr\n",
    "    if ir > 1:\n",
    "        diag0=0.0\n",
    "\n",
    "    ic=ir\n",
    "    while ic < nstat_sele-1:\n",
    "    \n",
    "        cor[ic-ir,ic]=diag0\n",
    "        cor[ic,ic-ir]=diag0\n",
    "    \n",
    "        ic+=1\n",
    "        \n",
    "    ir+=1\n",
    "\n",
    "\n",
    "cor[0,0]=1.0\n",
    "cor[nstat_sele-1,nstat_sele-1]=1.0\n",
    "\n",
    "ir=0\n",
    "ic=0\n",
    "while ir < nstat_sele:\n",
    "    ic=0\n",
    "    while ic < nstat_sele:\n",
    "        icov[ir,ic]=(1.0/(1.0-(r_corr**2)))*(1.0/(std0*std0))*cor[ir,ic]\n",
    "        ic += 1\n",
    "    ir += 1\n",
    "    \n",
    "print('\\n PRE-PROCESSING of the INVERSE COV MATRIX: \\n')\n",
    "print(cor,icov)\n",
    "print('\\n INVERSE of an EXPONENTIAL COV MATRIX should be tri-diagonal \\n')\n",
    "\n",
    "#######################################################\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "\n",
    "#\n",
    "# Initialize vectors for collecting sampled models\n",
    "#\n",
    "ALL_X = []\n",
    "ALL_Y = []\n",
    "ALL_Z = []\n",
    "ALL_T = []\n",
    "ALL_VP = []\n",
    "\n",
    "#\n",
    "#\n",
    "# (1) Campionamento dello spazio dei parametri (ripetuto N volte per M catene di Markov diverse)\n",
    "#\n",
    "ichain = 1 \n",
    "while ichain <= N_CHAINS:\n",
    "    \n",
    "    #\n",
    "    # (2) DEFINE STARTING MODEL as a random point in the model-space\n",
    "    #\n",
    "    X0, Y0, Z0, T0, VP0 = npa_get_starting_model(rng, xmin, xmax, ymin, ymax, zmin, zmax, tmin, tmax, vpmin, vpmax)\n",
    "    #\n",
    "    print('\\n%s\\n%s%8.2f%s%8.2f%s%8.2f%s%8.2f%s%8.2f\\n' % (\"STARTING MODEL:\",  \" - X: \", X0, \" - Y: \", Y0, \" - Z: \", Z0, \" - T0: \", T0, \" - Vp: \", VP0)) \n",
    "    #\n",
    "    # (3.1) COMPUTE SYNTHETICS for starting model\n",
    "    #\n",
    "    SYN_data_P = []\n",
    "    SYN_data_P = npa_LOCATE_syn(nstat_sele, stlo, stla, X0, Y0, Z0, T0, VP0)\n",
    "    #\n",
    "    # Write out last synthetics -- for a synthetic test\n",
    "    #\n",
    "    if N_MOD_MCMC == 1 and ichain == 1:\n",
    "        istat=0\n",
    "        while istat < nstat_sele:\n",
    "            x1=stlo[istat]\n",
    "            y1=stla[istat]\n",
    "            #theo_P=SYN_data_P[istat]   # SYNTHETICS without noise\n",
    "            theo_P=SYN_data_P[istat]+rng.normal(0.0,std0)  # SYNTHETICS with ADDED noise\n",
    "            print(\"%8d%8d%10.6f%10.1f%10.1f\" % (istat,istat,theo_P,x1,y1))\n",
    "            istat += 1\n",
    "\n",
    "    #\n",
    "    # (3.2) COMPUTE FIT TO STARTING MODEL\n",
    "    #\n",
    "    lppd0 = npa_log_likelihood(nstat_sele, SYN_data_P, Ppick, icov)\n",
    "    #\n",
    "    #\n",
    "    print('\\n LOG LIKELIHOOD starting model: ', lppd0)\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    #    START MCMC\n",
    "    #\n",
    "    #\n",
    "\n",
    "    imod = 1\n",
    "    while imod <= N_MOD_MCMC:\n",
    "    #\n",
    "    #\n",
    "\n",
    "\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    # (4.1) Propose a candidate model as a small preturbation of the current model\n",
    "    #\n",
    "    #\n",
    "        X_cand, Y_cand, Z_cand, T_cand, VP_cand = npa_candidate_model(rng, X0, Y0, Z0, T0, VP0, xmin, xmax, ymin, ymax, zmin, zmax, tmin, tmax, vpmin, vpmax)\n",
    "    #\n",
    "    #\n",
    "\n",
    "        #print(\"%s%10.3f%10.3f%10.3f\" % ('\\n   CURRENT MODEL: ', X0, Y0, Z0))\n",
    "        #print(\"%s%10.3f%10.3f%10.3f\" % (' CANDIDATE MODEL: ', X_cand, Y_cand, Z_cand))\n",
    "\n",
    "    #\n",
    "    #\n",
    "    #  (4.2) Compute predicted data given by the candidate model\n",
    "    #\n",
    "    #\n",
    "        SYN_data_P = []\n",
    "        SYN_data_P = npa_LOCATE_syn(nstat_sele, stlo, stla, X_cand, Y_cand, Z_cand, T_cand, VP_cand)\n",
    "    #\n",
    "    #\n",
    "\n",
    "    #\n",
    "    #\n",
    "    # (4.3) Compute the Likelihood between observed data and predicted data\n",
    "    #\n",
    "    #\n",
    "        lppd = npa_log_likelihood(nstat_sele, SYN_data_P, Ppick,  icov)\n",
    "\n",
    "        if PRIOR == 1:\n",
    "            lppd0 = 1.0\n",
    "            lppd  = 1.0\n",
    "   \n",
    "    #\n",
    "    #\n",
    "    # (4.4) Apply Metropolis'rule to select between candidate and current model\n",
    "    #\n",
    "    #\n",
    "        Accepted = npa_metropolis(rng, lppd,lppd0)\n",
    "    #\n",
    "    #\n",
    "        if imod % 1000 == 0:\n",
    "            print(\"%s%4d%s%9d%s%4d%s%16.2f%16.2f\" % ('CHAIN: ', ichain, ' SAMPLED MODEL: ', imod, '  Accepted:', Accepted, '  LPPD: ', lppd, lppd0))\n",
    "\n",
    "\n",
    "\n",
    "    #\n",
    "    #\n",
    "    # (4.5) Store the selected model for post-processing\n",
    "    #\n",
    "    #\n",
    "    # If the candidate model is rejected, current model becomes candidate model\n",
    "    #\n",
    "        if Accepted == 0:\n",
    "            \n",
    "            X_cand = X0\n",
    "            Y_cand = Y0\n",
    "            Z_cand = Z0\n",
    "            T_cand = T0\n",
    "            VP_cand = VP0\n",
    "            lppd = lppd0\n",
    "\n",
    "    #\n",
    "    # Store candidate model (after BURN-IN phase)\n",
    "    #\n",
    "\n",
    "        if imod >= BURN_IN:\n",
    "            \n",
    "            if imod % 100 == 0:\n",
    "            \n",
    "                ALL_X.append(X_cand)\n",
    "                ALL_Y.append(Y_cand)\n",
    "                ALL_Z.append(Z_cand)\n",
    "                ALL_VP.append(VP_cand)\n",
    "                ALL_T.append(T_cand)\n",
    "\n",
    "    #\n",
    "    # Candidate model becomes current model\n",
    "    #\n",
    "\n",
    "        X0 = X_cand\n",
    "        Y0 = Y_cand\n",
    "        Z0 = Z_cand\n",
    "        T0 = T_cand\n",
    "        VP0 = VP_cand\n",
    "        lppd0 = lppd\n",
    "\n",
    "\n",
    "    # Move McMC chain\n",
    "        imod += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Run a new indipendent McMC sampling\n",
    "    ichain += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing: compute posterior mean values of the investigated parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVER_X = np.mean(ALL_X,axis=0)\n",
    "STD_X = np.std(ALL_X,axis=0)\n",
    "\n",
    "AVER_Y = np.mean(ALL_Y,axis=0)\n",
    "STD_Y = np.std(ALL_Y,axis=0)\n",
    "\n",
    "AVER_Z = np.mean(ALL_Z,axis=0)\n",
    "STD_Z = np.std(ALL_Z,axis=0)\n",
    "\n",
    "AVER_T = np.mean(ALL_T,axis=0)\n",
    "STD_T = np.std(ALL_T,axis=0)\n",
    "\n",
    "AVER_VP = np.mean(ALL_VP,axis=0)\n",
    "STD_VP = np.std(ALL_VP,axis=0)\n",
    "\n",
    "\n",
    "print('\\n MEAN PPD  and STD PPD: \\n')\n",
    "print('%20s%10.2f%10.4f%25s%10.2f%10.4f' % ('X: ', AVER_X, STD_X, ' PRIOR MEAN/STD: ', 0.5*(xmin+xmax), (xmax-xmin)/sqrt(12)) )\n",
    "print('%20s%10.2f%10.4f%25s%10.2f%10.4f' % ('Y: ', AVER_Y, STD_Y, ' PRIOR MEAN/STD: ', 0.5*(ymin+ymax), (ymax-ymin)/sqrt(12)) )\n",
    "print('%20s%10.2f%10.4f%25s%10.2f%10.4f' % ('Z: ', AVER_Z, STD_Z, ' PRIOR MEAN/STD: ', 0.5*(zmin+zmax), (zmax-zmin)/sqrt(12)) )\n",
    "print('%20s%10.4f%10.4f%25s%10.4f%10.4f' % ('T: ', AVER_T, STD_T, ' PRIOR MEAN/STD: ', 0.5*(tmin+tmax), (tmax-tmin)/sqrt(12)) )\n",
    "print('%20s%10.2f%10.4f%25s%10.2f%10.4f' % ('Vp: ', AVER_VP, STD_VP, ' PRIOR MEAN/STD: ', 0.5*(vpmin+vpmax), (vpmax-vpmin)/sqrt(12)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing: 1D PPD  of the investigated parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3,ax4,ax5) = plt.subplots(1, 5, figsize=(15, 3))\n",
    "ax1.title.set_text('1D PPD X' )\n",
    "mu = AVER_X\n",
    "sigma = STD_X\n",
    "n, bins, patches = ax1.hist(ALL_X, 40, density=True)\n",
    "y = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n",
    "     np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n",
    "ax1.plot(bins,y,'--')\n",
    "ax1.set_xlim(xmin, xmax)\n",
    "ax1.set_xlabel('Longitude (deg)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "ax2.title.set_text('1D PPD Y' )\n",
    "mu = AVER_Y\n",
    "sigma = STD_Y\n",
    "n, bins, patches = ax2.hist(ALL_Y, 40, density=True)\n",
    "y = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n",
    "     np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n",
    "ax2.plot(bins,y,'--')\n",
    "ax2.set_xlim(ymin, ymax)\n",
    "ax2.set_xlabel('Latitude (deg)')\n",
    "\n",
    "ax3.title.set_text('1D PPD Z' )\n",
    "mu = AVER_Z\n",
    "sigma = STD_Z\n",
    "n, bins, patches = ax3.hist(ALL_Z, 40, density=True)\n",
    "y = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n",
    "     np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n",
    "ax3.plot(bins,y,'--')\n",
    "ax3.set_xlim(zmin,zmax)\n",
    "ax3.set_xlabel('Depth (km)')\n",
    "\n",
    "ax4.title.set_text('1D PPD Vp' )\n",
    "mu = AVER_VP\n",
    "sigma = STD_VP\n",
    "n, bins, patches = ax4.hist(ALL_VP, 40, density=True)\n",
    "y = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n",
    "     np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n",
    "ax4.plot(bins,y,'--')\n",
    "ax4.set_xlim(vpmin,vpmax)\n",
    "ax4.set_xlabel('Vp (m/s)')\n",
    "\n",
    "ax5.title.set_text('1D PPD T0' )\n",
    "mu = AVER_T\n",
    "sigma = STD_T\n",
    "n, bins, patches = ax5.hist(ALL_T, 40, density=True)\n",
    "y = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n",
    "     np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n",
    "ax5.plot(bins,y,'--')\n",
    "ax5.set_xlim(tmin,tmax)\n",
    "ax5.set_xlabel('Vs (m/s)')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing: 2D PPD  of the investigated parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "fig = plt.figure(figsize=(16.2,8))\n",
    "gs = gridspec.GridSpec(nrows=1, ncols=2, wspace=0.2, width_ratios=[1,1])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.title.set_text('XY 2D PPD Map of solutios' )\n",
    "ax1.plot(ALL_X, ALL_Y, '.', markersize=10)\n",
    "ax1.set_xlim(13,17)\n",
    "ax1.set_ylim(4,8)\n",
    "ax1.grid()\n",
    "ax1.plot(event_X, event_Y,marker='*', markersize=20, markerfacecolor='yellow')\n",
    "ax1.set_ylabel('Y (m)')\n",
    "ax1.set_xlabel('X (m)')\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.title.set_text('XZ 2D PPD Map of solutions' )\n",
    "ax2.plot(ALL_X, ALL_Z, '.', markersize=10)\n",
    "ax2.set_xlim(10,20)\n",
    "ax2.set_ylim(-9.5,0.5)\n",
    "ax2.grid()\n",
    "ax2.plot(event_X, 0.0,marker='*', markersize=20, markerfacecolor='yellow')\n",
    "ax2.set_ylabel('Z (m)')\n",
    "ax2.set_xlabel('X (m)')\n",
    "\n",
    "\n",
    "outputFigure=\"MCMC_results.jpg\"\n",
    "plt.savefig(outputFigure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
